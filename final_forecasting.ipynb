{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prophet import Prophet\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('df_hol.csv',low_memory=False,parse_dates=['date','min','max'])\n",
    "test=pd.read_csv('test_data_hol.csv',low_memory=False,parse_dates=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillMissingDates(df_df,unique_cols,frequency,freq,grain_with_time,sales_column,date_index=False):\n",
    "    date_ranges = df_df.groupby(unique_cols).apply(\n",
    "            lambda group: pd.date_range(group[frequency].min(), group[frequency].max(), freq=freq)\n",
    "        ).reset_index(name=frequency)\n",
    "\n",
    "\n",
    "    # Step 4: Create a new DataFrame that combines item-location combinations with the generated date ranges\n",
    "    date_ranges = date_ranges.explode(frequency).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Step 5: Merge the generated date ranges with the original data to get sales_quantity for each date\n",
    "    df_filled = pd.merge(date_ranges, df_df[grain_with_time+sales_column],\n",
    "                         on=unique_cols+[frequency], how='left')\n",
    "\n",
    "    # Step 6: Fill missing sales_quantity values with 0\n",
    "    df_filled[sales_column] = df_filled[sales_column].fillna(0)\n",
    "    if date_index:\n",
    "        return df_filled\n",
    "    # Step 7: Optionally, reset index if you want\n",
    "    else:\n",
    "        df_filled.reset_index(drop=True, inplace=True)\n",
    "        return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_for_group(df,groupno,group_value=None):\n",
    "    if groupno:\n",
    "        new_data=df[df['default_rank']==groupno]\n",
    "    elif group_value:\n",
    "        new_data=df[df['comb']==group_value]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyAutoArima(data,x=None):\n",
    "    arima_model=auto_arima(y=data,\n",
    "    X=x,\n",
    "    start_p=2,\n",
    "    d=None,\n",
    "    start_q=2,\n",
    "    max_p=10,\n",
    "    max_d=10,\n",
    "    max_q=10,\n",
    "    start_P=1,\n",
    "    D=None,\n",
    "    start_Q=1,\n",
    "    max_P=10,\n",
    "    max_D=10,\n",
    "    max_Q=10,\n",
    "    max_order=5,\n",
    "    m=1,\n",
    "    seasonal=True,\n",
    "    stationary=False,\n",
    "    information_criterion='aic',\n",
    "    alpha=0.05,\n",
    "    test='kpss',\n",
    "    seasonal_test='ocsb',\n",
    "    stepwise=False,\n",
    "    n_jobs=-1,\n",
    "    start_params=None,\n",
    "    trend=None,\n",
    "    method='lbfgs',\n",
    "    maxiter=500,\n",
    "    offset_test_args=None,\n",
    "    seasonal_test_args=None,\n",
    "    suppress_warnings=True,\n",
    "    error_action='trace',\n",
    "    trace=False,\n",
    "    random=False,\n",
    "    random_state=None,\n",
    "    n_fits=100,\n",
    "    return_valid_fits=False,\n",
    "    out_of_sample_size=0,\n",
    "    scoring='mse',\n",
    "    scoring_args=None,\n",
    "    with_intercept=\"auto\")\n",
    "    return arima_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Predict(train,test,groupno):\n",
    "    # filter the train and test data\n",
    "    train_data=filter_data_for_group(train,groupno)\n",
    "    test_data=filter_data_for_group(test,groupno)\n",
    "\n",
    "    #filter required columns\n",
    "    req_columns_train=['default_rank','comb','date','sales','onpromotion','holiday_true']\n",
    "    req_columns_test=['default_rank','comb','date','onpromotion','holiday_true']\n",
    "    train_data_f=train_data[req_columns_train]\n",
    "    test_data_f=test_data[req_columns_test]\n",
    "\n",
    "    # change holiday data into ints\n",
    "    train_data_f.loc[:,'holiday_true']=train_data_f['holiday_true'].astype('int64')\n",
    "    test_data_f.loc[:,'holiday_true']=test_data_f['holiday_true'].astype('int64')\n",
    "\n",
    "    #fill missing dates\n",
    "    train_data_ff=fillMissingDates(train_data_f,unique_cols=['default_rank'],frequency='date',freq='D',grain_with_time=['default_rank','date'],sales_column=['sales','onpromotion','holiday_true'])\n",
    "    test_data_ff=fillMissingDates(test_data_f,unique_cols=['default_rank'],frequency='date',freq='D',grain_with_time=['default_rank','date'],sales_column=['onpromotion','holiday_true'])\n",
    "\n",
    "\n",
    "    # min max scaler\n",
    "\n",
    "    arima_scaler_sales = MinMaxScaler()\n",
    "    arima_scaler_onpromotion = MinMaxScaler()\n",
    "    \n",
    "    train_data_ff['sales'] = arima_scaler_sales.fit_transform(train_data_ff[['sales']])\n",
    "    # test_data_ff['sales'] = arima_scaler_sales.transform(test_data_ff[['sales']])\n",
    "    \n",
    "    train_data_ff['onpromotion'] = arima_scaler_onpromotion.fit_transform(train_data_ff[['onpromotion']])\n",
    "    test_data_ff['onpromotion'] = arima_scaler_onpromotion.transform(test_data_ff[['onpromotion']])\n",
    "    \n",
    "    \n",
    "    train_data_fff=train_data_ff.set_index('date')[['sales']]\n",
    "    x=train_data_ff.set_index('date')[['holiday_true','onpromotion']]\n",
    "    \n",
    "    # handle the frq of the train and test data and exog variables data   \n",
    "    train_data_fff['group']=1\n",
    "    train_data_fff.reset_index(inplace=True)\n",
    "    train_data_ffff=fillMissingDates(train_data_fff,unique_cols=['group'],frequency='date',freq='D',grain_with_time=['group','date'],sales_column=['sales'])\n",
    "    train_data_ffff.set_index('date',inplace=True)\n",
    "    train_data_ffff.index = pd.to_datetime(train_data_ffff.index)\n",
    "    if train_data_ffff.index.duplicated().any():\n",
    "        print(\"Duplicate entries found. Resolving...\")\n",
    "        train_data_ffff = train_data_ffff[~train_data_ffff.index.duplicated(keep='first')]\n",
    "    train_data_ffff.index.freq='D'\n",
    "    train_data_ffff=train_data_ffff[['sales']]\n",
    "    \n",
    "\n",
    "    x['group']=1\n",
    "    x.reset_index(inplace=True)\n",
    "    x=fillMissingDates(x,unique_cols=['group'],frequency='date',freq='D',grain_with_time=['group','date'],sales_column=['holiday_true','onpromotion'])\n",
    "    x.set_index('date',inplace=True)\n",
    "    x.index = pd.to_datetime(x.index)\n",
    "    if x.index.duplicated().any():\n",
    "        print(\"Duplicate entries found. Resolving...\")\n",
    "        x = x[~x.index.duplicated(keep='first')]\n",
    "    x.index.freq='D'\n",
    "    x=x[['holiday_true','onpromotion']]\n",
    "\n",
    "    \n",
    "    x_test=test_data_ff.set_index('date')[['holiday_true','onpromotion']]\n",
    "\n",
    "    arima_model=MyAutoArima(train_data_ffff,x)\n",
    "    res=arima_model.fit(y=train_data_ffff,X=x)\n",
    "    \n",
    "    start = len(train_data_ffff)\n",
    "    end = start + len(x_test) - 1\n",
    "    \n",
    "    assert x.shape[1] == x_test.shape[1], \"Mismatch in number of columns between training and test exogenous variables.\"\n",
    "    assert len(x_test) == (end - start + 1), \"Mismatch between prediction period and test exogenous variables.\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Predict with exogenous variables\n",
    "    pred_data = res.predict(n_periods=len(x_test), X=x_test).rename('sales')\n",
    "\n",
    "\n",
    "    predicted_df=pd.DataFrame(pred_data)\n",
    "\n",
    "    predicted_df['sales']=arima_scaler_sales.inverse_transform(predicted_df[['sales']]) \n",
    "\n",
    "    predicted_df=predicted_df.reset_index().rename(columns={'index':'date'})\n",
    "\n",
    "    predicted_df=test_data[['id','date']].merge(predicted_df,on='date',how='left')\n",
    "    predicted_df['Group']=groupno\n",
    "    \n",
    "    # # Output predictions\n",
    "    return res,x_test,end,start,predicted_df,train_data_f,test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Predict_Prophet(train,test,groupno):\n",
    "    # filter the train and test data\n",
    "    train_data=filter_data_for_group(train,groupno)\n",
    "    test_data=filter_data_for_group(test,groupno)\n",
    "\n",
    "    #filter required columns\n",
    "    req_columns_train=['default_rank','comb','date','sales','onpromotion','holiday_true']\n",
    "    req_columns_test=['default_rank','comb','date','onpromotion','holiday_true']\n",
    "    train_data_f=train_data[req_columns_train]\n",
    "    test_data_f=test_data[req_columns_test]\n",
    "\n",
    "    # change holiday data into ints\n",
    "    train_data_f.loc[:,'holiday_true']=train_data_f['holiday_true'].astype('int64')\n",
    "    test_data_f.loc[:,'holiday_true']=test_data_f['holiday_true'].astype('int64')\n",
    "\n",
    "    #fill missing dates\n",
    "    train_data_ff=fillMissingDates(train_data_f,unique_cols=['default_rank'],frequency='date',freq='D',grain_with_time=['default_rank','date'],sales_column=['sales','onpromotion','holiday_true'])\n",
    "    test_data_ff=fillMissingDates(test_data_f,unique_cols=['default_rank'],frequency='date',freq='D',grain_with_time=['default_rank','date'],sales_column=['onpromotion','holiday_true'])\n",
    "\n",
    "\n",
    "    # min max scaler\n",
    "\n",
    "    Prophet_scaler_sales = MinMaxScaler()\n",
    "    Prophet_scaler_onpromotion = MinMaxScaler()\n",
    "    \n",
    "    train_data_ff['sales'] = Prophet_scaler_sales.fit_transform(train_data_ff[['sales']])\n",
    "    # test_data_ff['sales'] = Prophet_scaler_sales.transform(test_data_ff[['sales']])\n",
    "    \n",
    "    train_data_ff['onpromotion'] = Prophet_scaler_onpromotion.fit_transform(train_data_ff[['onpromotion']])\n",
    "    test_data_ff['onpromotion'] = Prophet_scaler_onpromotion.transform(test_data_ff[['onpromotion']])\n",
    "\n",
    "    # convert_columns into standard names\n",
    "\n",
    "    train_data_ff.rename(columns={'date':'ds','sales':'y'},inplace=True)\n",
    "    test_data_ff.rename(columns={'date':'ds'},inplace=True)\n",
    "    print(train_data_ff.columns)\n",
    "    \n",
    "    # define the model\n",
    "\n",
    "    model=Prophet(daily_seasonality=True)\n",
    "    # model.add_regressor('holiday_true')\n",
    "    # model.add_regressor('onpromotion')\n",
    "    model.fit(train_data_ff)\n",
    "    future_dfs=model.make_future_dataframe(periods=len(test_data_ff))\n",
    "    \n",
    "    \n",
    "    future_dfs = future_dfs.merge(\n",
    "    test_data_ff[['ds', 'holiday_true', 'onpromotion']],\n",
    "    on='ds',\n",
    "    how='left'\n",
    "    )\n",
    "    future_dfs.fillna({'holiday_true': 0, 'onpromotion': 0}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Predict with exogenous variables\n",
    "    future_dfs=future_dfs.tail(len(test_data_ff)).head(len(test_data_ff))\n",
    "    preds=model.predict(future_dfs)\n",
    "\n",
    "    preds['yhat'] = Prophet_scaler_sales.inverse_transform(preds[['yhat']])\n",
    "    preds['yhat_lower'] = Prophet_scaler_sales.inverse_transform(preds[['yhat_lower']])\n",
    "    preds['yhat_upper'] = Prophet_scaler_sales.inverse_transform(preds[['yhat_upper']])\n",
    "    \n",
    "    #\n",
    "    \n",
    "    predicted_df=preds[['ds','yhat','yhat_upper','yhat_lower']]\n",
    "    predicted_df=test_data[['id','date']].merge(predicted_df,right_on='ds',left_on='date',how='left')\n",
    "    predicted_df['Group']=groupno\n",
    "    \n",
    "    # # Output predictions\n",
    "    return model,future_dfs,predicted_df,train_data_f,test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data periodically\n",
    "def save_batch_data(data_list, save_path, batch_number):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)  # Create directory if it doesn't exist\n",
    "    \n",
    "    combined_df = pd.concat(data_list, ignore_index=True)\n",
    "    file_name = f\"predicted_batch_prophet_v1_{batch_number}.csv\"\n",
    "    file_path = os.path.join(save_path, file_name)\n",
    "    combined_df.to_csv(file_path, index=False)\n",
    "    print(f\"Batch {batch_number} saved to {file_path}\")\n",
    "\n",
    "# Main processing loop with tqdm for progress bar\n",
    "def process_all_groups(df_hol, test_data_hol, all_groups, save_interval=100, save_path='predicted_arima_data_v2'):\n",
    "    all_predicted_data = []\n",
    "    batch_counter = 0  # To track number of processed batches\n",
    "\n",
    "    # Loop through groups with tqdm\n",
    "    for i, group in enumerate(tqdm.tqdm(all_groups, desc=\"Processing Groups\"), start=1):\n",
    "        # Process group data\n",
    "        res, x_test, end, start, predicted_df, train_data_f, test_data = Train_Predict(df_hol, test_data_hol, group)\n",
    "        all_predicted_data.append(predicted_df)\n",
    "        \n",
    "        # Check if it's time to save the data\n",
    "        if i % save_interval == 0:\n",
    "            batch_counter += 1\n",
    "            save_batch_data(all_predicted_data, save_path, batch_counter)\n",
    "            all_predicted_data.clear()  # Clear the list after saving to free memory\n",
    "\n",
    "    # Save remaining data if any after the loop ends\n",
    "    if all_predicted_data:\n",
    "        batch_counter += 1\n",
    "        save_batch_data(all_predicted_data, save_path, batch_counter)\n",
    "\n",
    "def process_all_groups_Prophet(df_hol, test_data_hol, all_groups, save_interval=100, save_path='predicted_prophet_data_v1'):\n",
    "    all_predicted_data = []\n",
    "    batch_counter = 0  # To track number of processed batches\n",
    "\n",
    "    # Loop through groups with tqdm\n",
    "    for i, group in enumerate(tqdm.tqdm(all_groups, desc=\"Processing Groups\"), start=1):\n",
    "        # Process group data\n",
    "        model,future_dfs,predicted_df,train_data_f,test_data = Train_Predict_Prophet(df_hol, test_data_hol, group)\n",
    "        all_predicted_data.append(predicted_df)\n",
    "        \n",
    "        # Check if it's time to save the data\n",
    "        if i % save_interval == 0:\n",
    "            batch_counter += 1\n",
    "            save_batch_data(all_predicted_data, save_path, batch_counter)\n",
    "            all_predicted_data.clear()  # Clear the list after saving to free memory\n",
    "\n",
    "    # Save remaining data if any after the loop ends\n",
    "    if all_predicted_data:\n",
    "        batch_counter += 1\n",
    "        save_batch_data(all_predicted_data, save_path, batch_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups=train.default_rank.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call():\n",
    "    if __name__ == '__main__':\n",
    "          # Assuming 'group' is a column in df_hol\n",
    "        process_all_groups_Prophet(train, test, all_groups, save_interval=500, save_path=\"predicted_prophet_data_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_safekeep/df_hol.csv',low_memory=False,parse_dates=['date','min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
